pblk: Physical Block Device Target

pblk is a LightNVM target implementing a full host-side Flash Translation Layer
(FTL). As a target, pblk presents itself as a make_request_fn base driver to the
block layer. In this particular case, as a block device driver, which exposes
the Open-Channel SSD storage device as a block storage to user space. In terms
of target functionality, pblk implements everything related to data placement,
garbage collection (GC) and recovery. In order to access the physical flash,
pblk makes use of LightNVM's generic media manager, which exposes a get/put
block provisioning interface.

By design, pblk supports pluggable mapping and GC algorithms so that a single
hardware device can be accommodated for specific workloads by simply making
changes in software to the desired module (e.g., to the data stripping strategy
in charge of mapping logical to physical addresses).

At the moment, pblk implements the following functionality:

  1. Sector-based host-side translation table management
  2. Write buffering for writing to physical flash pages
  3. Translation table recovery on power outage
  4. Retire bad flash blocks on write failure
  5. Simple cost-based garbage collector (GC)
  6. Sysfs integration
  7. I/O rate-limiting ??


1. Sector-based host-side translation table management
======================================================

 Mapping Strategy
 ----------------


 2. Write Buffer
 ===============
 pblk operates around a ring write buffer of size N, where N is the closest
 power-of-2 size to the number of active LUNs in the target times the size of a
 flash block. 

 Apart from the performance benefit of buffering writes, this buffer allows to
 respect controller constrains on how flash pages must be written. Typically,
 flash pages are between 16KB and 32KB large, and the controller might introduce
 other abstractions that tight pages together (e.g., planes) and make "apparent"
 page sizes to be fairly large (e.g., 64KB, 128KB). These constrains must be
 respected when sending write I/Os to the controller.

 Moreover, some flash media such as MLC flash are organized in page pairs (lower
 and upper pages), which must be written in order for a write to be guaranteed
 persistence. In order words, the data in the lower flash page cannot be read
 back until the upper flash page has been written. Therefore, even if data has
 been written, it must still reside in host until more data has been written. We
 refer to the number of sectors that must reside in the write buffer before it
 is safe to read from the media as the buffers "grace area". We describe below
 how we implement this grace area through a dedicated pointer on the write
 buffer. This problem only aggravates with more complex media such as TLC or
 QLC.

 By buffering writes, both controller- and media-specific constrains can be met.

 Ring Write Buffer Design
 ------------------------
 Apart from the usual head (mem) and tail (subm) pointers, pblk's ring write
 buffer maintains a other pointers for different purposes. We describe each
 pointer separately:

   - Memory Pointer (mem): This is the head pointer. It points to the next
     writable entry on the buffer.
   - Submission Pointer (subm): This is the tail pointer. It points to the
     next buffered entry that is ready to be submitted to the media. Buffer
     space and count are calculated using mem and subm.
   - Sync Pointer (sync): It signals the last submitted entry that has
     completed; i.e., that has successfully been persisted to the media.
     It acts as a backpointer that guarantees that I/Os are completed in
     order. This is necessary to guarantee that flushes are completed
     correctly (See Documentation/block/writeback_cache_control.txt).
   - Flush Pointer (flush): It guarantees that flushes are respected
     by signaling the last entry associated to a REQ_FLUSH request.
     This pointer is taken into consideration by the write thread
     consuming the buffer.
   - Mapping Update Pointer (l2p_update): It guarantees that L2P
     lookups for entries that still reside on the write buffer
     cache will point to it, even if they have successfully been
     persisted to the media. By doing this we guarantee that all
     entries residing in the write buffer will be read from
     cache, thus preventing I/Os to be sent to the device to
     retrieve half-written flash pages. This way, we ensure that
     the whole buffer defines a grace area; the mapping table is
     only updated when the buffer head wraps up.

The write buffer is complemented with a write context buffer, which stores
metadata for each 4KB write. (XXX: Mapping strategy) As mentioned above, we
follow a late-map approach, where the actual mapping to flash pages is done when
the write buffer is being written to the media. This allows us to decouple the
mapping strategy from buffer writes, thus enabling mapping optimizations for
different workloads.

To minimize calculations, pblk's write buffer must be of a power-of-2 size...
also write that it is the closes one to the number of active LUNs in the target
X the size of a flash block.

          mem     l2p_update       sync                        subm
           |          |             |                            |
 -------------------------------------------------------------------------------
|          |          |   synced    |          submitted                       |
-------------------------------------------------------------------------------
| CCCCCCCC | DDDDDDD  | CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC|
 -------------------------------------------------------------------------------

C: L2P points to cache
D: L2P points to device


Write Path
----------
On a new write, the bio associated to it is converted into a write context
(pblk_w_ctx), where the logical address (lba), length and flush requirements are
stored. The bio data is copied to the internal write buffer and completed if a
flush is not required (in case of a flush, the bio is completed after all data
on the write buffer has been persisted to the media). As data is stored on the
write buffer, the translation table is updated so that lbas point to the buffer
entries data has been stored in.

A separate write thread consumes user data from the write buffer and forms new
bios that are then sent to the device. The mapping strategy described in Section
1 takes place here. This strategy can be modified, as long as controller
constrains are considered. If a flush is required, but there is not enough data
on the write buffer to fulfill these constrains, padding is performed. If the
write succeeds, the sync pointer will be updated on the completion path. When
the head pointer (mem) wraps up and overwrites old data, the l2p_update pointer
takes care of updating the translation table so that future lookups will point
to physical addresses on the device. As mentioned, this late mapping to device
addresses is the mechanism that we use to guarantee the buffer's grace area.

Note writes might fail. Look at Section 4 for an explanation on how to deal with
this case.

Read Path
---------
The read path is simpler than the write path. For each 4KB sector on the read
bio, a lookup is performed to find if data is cached on the write buffer or if
it is necessary to submit an I/O to retrieve it from the physical flash. It
might happen that only some sectors reside in cache. In this case, a new bio is
issue to retrieve sectors from the media, and the original bio is filled out
manually with them.


3. Translation table recovery
=============================
The L2P translation table is maintained entirely on the host memory. The ratio
is approximately 1GB of L2P per 1TB of flash. In order to recover the
translation table in case of power failure, pblk maintains different levels of
redundancy. Firstly, for each 4KB sector, the lba to ppa mapping is stored in
the out-of-bound (OOB) area for that sector. Secondly, when a block is closed,
the last page contains metadata on the block itself, including the lba - ppa
mapping list, block status and counters. Thirdly, a L2P snapshot is stored on
the media on shutdown. The first two methods allow to recover using different
levels of scanning in case of a power failure. The last method is the typical
case, when the host is powered down gracefully.

All three methods are based on top of the generic media manager... TODO


4. Write Recovery
=================

5. Garbage Collection
=====================

6. Sysfs integration
====================

7. I/O rate-limiting ??
